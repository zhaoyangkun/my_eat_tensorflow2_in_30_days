{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备数据\n",
    "\n",
    "`imdb` 数据集的目标是根据电影评论的文本内容预测评论的情感标签。\n",
    "\n",
    "训练集有 `20000` 条电影评论文本，测试集有 `5000` 条电影评论文本，其中正面评论和负面评论都各占一半。\n",
    "\n",
    "文本数据预处理较为繁琐，包括中文切词（本示例不涉及），构建词典，编码转换，序列填充，构建数据管道等等。\n",
    "\n",
    "在 `tensorflow` 中完成文本数据预处理的常用方案有两种，第一种是利用 `tf.keras.preprocessing` 中的 `Tokenizer` 词典构建工具和 `tf.keras.utils.Sequence` 构建文本数据生成器管道。\n",
    "\n",
    "第二种是使用 `tf.data.Dataset` 搭配 `.keras.layers.experimental.preprocessing.TextVectorization` 预处理层。\n",
    "\n",
    "第一种方法较为复杂，其使用范例可以参考以下文章。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/67697840\n",
    "\n",
    "第二种方法为 `TensorFlow` 原生方式，相对也更加简单一些。\n",
    "\n",
    "我们此处介绍第二种方法。\n",
    "\n",
    "![](./data/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time', 'only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me', 'than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also', 'other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, preprocessing, optimizers, losses, metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re, string\n",
    "\n",
    "train_data_path = \"./data/imdb/train.csv\"\n",
    "test_data_path = \"./data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的 10000 个词\n",
    "MAX_LEN = 200  # 每个样本仅保留 200 个词的长度\n",
    "BATCH_SIZE = 20  # 每次训练 20 个样本\n",
    "\n",
    "# 构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line, \"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]), tf.int32), axis=0)\n",
    "    text = tf.expand_dims(arr[1], axis=0)\n",
    "    return (text, label)\n",
    "\n",
    "\n",
    "ds_train_raw = (\n",
    "    tf.data.TextLineDataset(filenames=[train_data_path])\n",
    "    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_test_raw = (\n",
    "    tf.data.TextLineDataset(filenames=[test_data_path])\n",
    "    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 构建字典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    clean_punctuation = tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "    return clean_punctuation\n",
    "\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split=\"whitespace\",\n",
    "    max_tokens=MAX_WORDS - 1,  # 留一个给占位符\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN,\n",
    ")\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "# 单词编码\n",
    "ds_train = ds_train_raw.map(\n",
    "    lambda text, label: (vectorize_layer(text), label)\n",
    ").prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text, label: (vectorize_layer(text), label)).prefetch(\n",
    "    tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,) (1,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in ds_train.unbatch().take(1):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，定义模型\n",
    "\n",
    "使用 `Keras` 接口有以下 3 种方式构建模型：使用 `Sequential` 按层顺序构建模型，使用函数式 `API` 构建任意结构模型，继承 `Model` 基类构建自定义模型。\n",
    "\n",
    "此处选择使用继承 `Model` 基类构建自定义模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 7)            70000     \n",
      "                                                                 \n",
      " conv_1 (Conv1D)             (None, 196, 16)           576       \n",
      "                                                                 \n",
      " pool_1 (MaxPooling1D)       (None, 98, 16)            0         \n",
      "                                                                 \n",
      " conv_2 (Conv1D)             (None, 97, 128)           4224      \n",
      "                                                                 \n",
      " pool_2 (MaxPooling1D)       (None, 48, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6144)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,945\n",
      "Trainable params: 80,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16, kernel_size=5, name=\"conv_1\", activation=\"relu\")\n",
    "        self.pool_1 = layers.MaxPool1D(name=\"pool_1\")\n",
    "        self.conv_2 = layers.Conv1D(\n",
    "            128, kernel_size=2, name=\"conv_2\", activation=\"relu\"\n",
    "        )\n",
    "        self.pool_2 = layers.MaxPool1D(name=\"pool_2\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1, activation=\"sigmoid\")\n",
    "        super(CnnModel, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "    def summary(self):\n",
    "        x_input = layers.Input(shape=MAX_LEN)\n",
    "        output = self.call(x_input)\n",
    "        model = tf.keras.Model(inputs=x_input, outputs=output)\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "model = CnnModel()\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，训练模型\n",
    "\n",
    "训练模型通常有 `3` 种方法，内置 `fit` 方法，内置 `train_on_batch` 方法，以及自定义训练循环。此处我们通过自定义训练循环训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    today_ts = tf.timestamp() % (24 * 60 * 60)\n",
    "    hour = tf.cast(today_ts // 3600 + 8, tf.int32) % tf.constant(24)\n",
    "    minute = tf.cast((today_ts % 3600) // 60, tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts % 60), tf.int32)\n",
    "\n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\", m)) == 1:\n",
    "            return tf.strings.format(\"0{}\", m)\n",
    "        else:\n",
    "            return tf.strings.format(\"{}\", m)\n",
    "\n",
    "    timestring = tf.strings.join(\n",
    "        [timeformat(hour), timeformat(minute), timeformat(second)], separator=\":\"\n",
    "    )\n",
    "    tf.print(\"==========\" * 2 + timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 15:11:01.454338: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================15:11:08\n",
      "Epoch=1, Loss:0.446296602, Accuracy:0.76495, Valid Loss:0.320769042, Valid Accuracy:0.8624\n",
      "\n",
      "====================15:11:14\n",
      "Epoch=2, Loss:0.247373357, Accuracy:0.90055, Valid Loss:0.322131723, Valid Accuracy:0.868\n",
      "\n",
      "====================15:11:21\n",
      "Epoch=3, Loss:0.178686142, Accuracy:0.9301, Valid Loss:0.36509043, Valid Accuracy:0.865\n",
      "\n",
      "====================15:11:27\n",
      "Epoch=4, Loss:0.120583653, Accuracy:0.95685, Valid Loss:0.450968593, Valid Accuracy:0.86\n",
      "\n",
      "====================15:11:33\n",
      "Epoch=5, Loss:0.0750969499, Accuracy:0.97565, Valid Loss:0.612897396, Valid Accuracy:0.854\n",
      "\n",
      "====================15:11:37\n",
      "Epoch=6, Loss:0.044601161, Accuracy:0.98515, Valid Loss:0.780356705, Valid Accuracy:0.8544\n",
      "\n",
      "====================15:11:43\n",
      "Epoch=7, Loss:0.0249042232, Accuracy:0.9921, Valid Loss:0.970365345, Valid Accuracy:0.85\n",
      "\n",
      "====================15:11:48\n",
      "Epoch=8, Loss:0.0145032955, Accuracy:0.9957, Valid Loss:1.04256284, Valid Accuracy:0.8518\n",
      "\n",
      "====================15:11:54\n",
      "Epoch=9, Loss:0.0173363537, Accuracy:0.99425, Valid Loss:1.11363089, Valid Accuracy:0.8484\n",
      "\n",
      "====================15:12:01\n",
      "Epoch=10, Loss:0.0148880268, Accuracy:0.9949, Valid Loss:1.2527951, Valid Accuracy:0.8474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name=\"train_loss\")\n",
    "train_metric = metrics.BinaryAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "valid_loss = metrics.Mean(name=\"valid_loss\")\n",
    "valid_metric = metrics.BinaryAccuracy(name=\"valid_accuracy\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features, training=False)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "def train_model(model, ds_train, ds_valid, epochs):\n",
    "    for epoch in tf.range(1, epochs + 1):\n",
    "        for features, labels in ds_train:\n",
    "            train_step(model, features, labels)\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model, features, labels)\n",
    "\n",
    "        logs = \"Epoch={}, Loss:{}, Accuracy:{}, Valid Loss:{}, Valid Accuracy:{}\"\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            printbar()\n",
    "            tf.print(\n",
    "                tf.strings.format(\n",
    "                    logs,\n",
    "                    (\n",
    "                        epoch,\n",
    "                        train_loss.result(),\n",
    "                        train_metric.result(),\n",
    "                        valid_loss.result(),\n",
    "                        valid_metric.result(),\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            tf.print(\"\")\n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "\n",
    "train_model(model, ds_train, ds_test, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，评估模型\n",
    "\n",
    "通过自定义训练的模型没有经过编译，无法直接使用 `model.evaluate(ds_valid)` 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:1.2527951, Valid Accuracy:0.8474\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "        valid_step(model, features, labels)\n",
    "    logs = \"Valid Loss:{}, Valid Accuracy:{}\"\n",
    "    tf.print(tf.strings.format(logs, (valid_loss.result(), valid_metric.result())))\n",
    "    # print(logs.format(valid_loss.result(), valid_metric.result()))\n",
    "    valid_loss.reset_states()\n",
    "    valid_metric.reset_states()\n",
    "\n",
    "\n",
    "evaluate_model(model, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，使用模型\n",
    "\n",
    "可以使用以下方法:\n",
    "\n",
    "* `model.predict(ds_test)`\n",
    "* `model(x_test)`\n",
    "* `model.call(x_test)`\n",
    "* `model.predict_on_batch(x_test)`\n",
    "\n",
    "推荐优先使用 `model.predict(ds_test)` 方法，既可以对 `Dataset`，也可以对 `Tensor` 使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21379325],\n",
       "       [0.9999994 ],\n",
       "       [0.9999975 ],\n",
       "       ...,\n",
       "       [0.99999666],\n",
       "       [0.47219488],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.13793248e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99997497e-01]\n",
      " [1.43055489e-12]\n",
      " [9.99670744e-01]\n",
      " [2.58837136e-11]\n",
      " [2.78694867e-09]\n",
      " [2.80160157e-06]\n",
      " [1.00000000e+00]\n",
      " [9.99019980e-01]\n",
      " [1.00000000e+00]\n",
      " [7.03761399e-01]\n",
      " [3.84878284e-12]\n",
      " [9.99991775e-01]\n",
      " [1.94970307e-11]\n",
      " [2.77695153e-02]\n",
      " [1.42233901e-07]\n",
      " [9.98865128e-01]\n",
      " [1.21927485e-02]\n",
      " [9.99902487e-01]], shape=(20, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x_test, _ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    # 以下方法等价：\n",
    "    # print(model.call(x_test))\n",
    "    # print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，保存模型\n",
    "\n",
    "推荐使用 `TensorFlow` 原生方式保存模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 15:21:06.494532: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/tf_model_saved_model/assets\n",
      "export saved model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.21379325],\n",
       "       [0.9999994 ],\n",
       "       [0.9999975 ],\n",
       "       ...,\n",
       "       [0.99999666],\n",
       "       [0.47219488],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"./data/tf_model_saved_model\", save_format=\"tf\")\n",
    "print(\"export saved model.\")\n",
    "\n",
    "model_loaded = tf.keras.models.load_model(\"./data/tf_model_saved_model\")\n",
    "model_loaded.predict(ds_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2c115b8898d18cbad3dd22ca2ef56c09a93f1a7aecc0710612fc4d17026bb07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
